---
layout: page
title: Project Overview
permalink: /overview/
---

**Partners**: CDL, PLOS, DataONE
**Proposal**: [PDF on eScholarship](http://escholarship.org/uc/item/9kf081vf)
**NSF Grant Record**: [Grant No. 1448821](http://www.nsf.gov/awardsearch/showAward?AWD_ID=1448821&HistoricalAwards=false)
**Timeline**: 1 Oct 2014 - 1 Oct 2015


## Project Communication

* Internal documentation: Google docs - MDC project space 
* Public documentation: MDC Github pages 
* Progress update meetings with entire team at the end of each work unit: conference call
* Public announcements: organization blogs or news platforms
  * [CDL Data Pub](http://datapub.cdlib.org)
  * [PLOS Tech](http://blogs.plos.org/tech)
  * [DataONE News](https://www.dataone.org/news)
* Social media platform: Twitter
  * @plosalm
  * @UC3CDL
  * @DataONEorg

## Personnel

* CDL
  * Patricia Cruse (5%), Project oversight
  * John Kratz (in-kind), Field research
  * Carly Strasser (25%), PM for field research
* NCEAS / DataONE
  * Amber Budden (in-kind), DataONE Community Engagement
  * Matt Jones (in-kind), DataONE oversight, Manage development
  * Dave Vieglais (in-kind), DataONE oversight
  * Developer (NCEAS, TBD) (17%), DataONE integration
* PLOS
  * John Chodacki (in-kind), Product Director, PLOS oversight
  * Martin Fenner (100%), Technical Lead, DLM developer
  * Jennifer Lin (50%), PM
  * Kristen Ratan (in-kind), Publisher,  PLOS oversight


## Purpose

We propose to design and develop metrics that track and measure data use, “data-level metrics” (DLM). DLM are a multi-dimensional suite of indicators, measuring the broad range of activity surrounding the reach and use of data as a research output. We will investigate the validity and feasibility of the metrics by collecting them and investigating how to best make use of the data harvested to power discovery and reporting of scholarly these scholarly outputs.  

## High-level Project Summary

1. design, develop, and prototype a reference model for data metrics
1. test mechanisms of automatic tracking
1. explore ways in which the raw DLM data be delivered to drive data discovery across data types and research questions
1. prototype a flexible report for funders, institutions, and researchers to share DLM results.  

## Work Components

1. DLM Field Research (Oct-Jan)
  * What: Surveys, interviews, focus groups to determine requirements for DLM
  * Who: CDL with PLOS input
  * Input: list of questions/things we want to know from the community
  * Output: metrics design & requirements
2. Data Usage Tracking (Nov-Feb)
  * What: Extend DataONE usage tracking capacity 
  * Who: DataONE with PLOS input
  * Input: existing usage API
  * Output: extended usage API (COUNTER)
3. Data Activity Aggregation (Dec-Apr)
  * What: Formulate a set of metrics to text; extend technology
  * Who: PLOS
  * Input: design & requirements
  * Output: DLM application
4. DLM Integration & Presentation (May-Jun)
  * What:  Develop tools for the community to use metrics
  * Who: PLOS 
  * Input: ALM Reports source code, DLM data
  * Output: DLM Reports application & widgets
5. Bibliometric Analysis (July-Sep)
  * What: Analyze, write up results from project
  * Who: PLOS, CDL, DataONE
  * Input: data design and collection results
  * Output: final report and recommendations

## Final Project Report & Recommendations 

Report on Research Questions:
* DLM Generation
  * What metrics are valid, possible to collect through automatic tracking, and useful to the community?
  * What are the limitations and risks to the metrics identified in our pilot? Will they create or enhance any existing bias?
  * What are the community requirements for DLM data validity and reliability? Is gaming an issue and if so, how might it be addressed the metrics implementation?
  * What data channels and associated sources (web services) are sustainable for building a long-term DLM framework?
  * What data collection practices are needed to promote standardization of DLM data and ensure cross-comparison of harvested data?
* DLM Use (Data discovery, evaluation, and bibliometric analysis)
  * Who will use the metrics and in what manner (use cases & limitations)?
  * What do the metrics tell us and what are they not able to tell us?
  * Do research communities use data differently and what demographic differences are expressed (subject area, geography, institutional affiliation, etc.)?
  * How can the metrics be most effectively communicated and shared (aggregations and reporting recommendations)?
  * Which of the piloted metrics highlight differences between open data and closed data? Are data in compliance with open access policies and sharing distinguished in the piloted metrics?
  * What cyberinfrastructure gaps (metadata holes, system bridges, etc.) exist for DLM to be fully integrated into the larger research information ecosystem (with repository, funder, researcher, institution information)?
 
## Recommendations
We will make recommendations for future work based on the results of the pilot, addressing both social and technical areas. They will likely be addressed to the appropriate stakeholders, likely including infrastructure organizations, repositories/data centers, publishers, and funders.
